---
title: "STAT_680_HW2"
author: "Yudi Zhang, Wangqian Ju"
date: "10/12/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(bayesImageS)
require(MixSim)
library(zoo)
library(boot)

set.seed(1234)
```

```{r funcitons}
k_dens <- function(x, h, data) {
  n <- length(data)
  sum(dnorm((x - data) / h)) / n
}

n_peak <- function(data, h = 1, x_length.out = 100, xmin = -10, xmax = 30) {
  x <- seq(xmin, xmax, length.out = x_length.out)
  y <- sapply(x, k_dens, h = h, data = data)
  check.uni <- sum(zoo::rollapply(y, 3, function(x) {x[2] == max(x)}))
  return(check.uni)
}

bi_search <- function(data, lower, upper, tol = 10^-3){
  ll <- n_peak(data, lower)
  rr <- n_peak(data, upper)

  if(!(ll > 1 & rr == 1)) {
    stop("choose lower and upper again")
  }

  repeat{
    mid <- mean(c(lower, upper))
    mm <- n_peak(data, mid)

    if(mm > 1) {
      lower <- mid
    } else {
      upper <- mid
    }

    if(abs(lower - upper) < tol) break
  }

  return(upper)
}

boot_paulsen <- function(data, i, h) {
    x_star <- data[i] + h * rnorm(length(data))

    n_peak(x_star, h = h) > 1
}


get_n_rs_tbl <- function(tt, k, center) {
  cc <- combn(k, 2)
  nrstbl <- tibble(cluster1 = c(cc[1,], cc[2,]),
                   cluster2 = c(cc[2,], cc[1,]))
  nrstbl$l_center <- lapply(nrstbl$cluster2, function(idx) center[idx,])
  nrstbl$Xi <- lapply(nrstbl$cluster1, function(cluster) {
    tt$data[tt$cluster == cluster, ]
  })
  nrstbl <- nrstbl %>% mutate(
    n_rs_cluster = purrr::map2(l_center, Xi, function(cent, dd) {
      sapply(1:nrow(dd), function(idx) {
        sqrt(sum((dd[idx, ] - cent)^2))
      })
    })
  ) %>% unnest(n_rs_cluster)
  return(nrstbl)
}


# p3
find_neighbor <- function(z, i, j) {
  dim.z <- dim(z)

  if(i > dim.z[1] | j > dim.z[2]) return(NULL)

  count <- 0

  neigh.idx <- list()
  if(i - 1 >= 1) {
    count <- count + 1
    neigh.idx[[count]] <- c(i-1, j)
  }

  if(i + 1 <= dim.z[1]) {
    count <- count + 1
    neigh.idx[[count]] <- c(i + 1, j)
  }

  if(j - 1 >= 1) {
    count <- count + 1
    neigh.idx[[count]] <- c(i, j - 1)
  }

  if(j + 1 <= dim.z[2]) {
    count <- count + 1
    neigh.idx[[count]] <- c(i, j + 1)
  }

  return(neigh.idx)

}

compute_log_cond_prob <- function(z, i, j, alpha = 0, beta = 0) {
  xi <- z[i,j]
  xi_neighbor <- sapply(find_neighbor(z, i, j), function(nn) {
    z[nn[1], nn[2]]
  })

  beta_coef <- sum(xi == xi_neighbor)
  num <- xi * alpha + beta_coef * beta
  dem <- log(exp(num) + exp( (1-xi)*alpha + (length(xi_neighbor) - beta_coef)*beta ))

  return(num - dem)
}

compute_neg_log_prof_llh <- function(par, z) {
  alpha <- par[1]
  beta <- par[2]

  result <- sapply(1:nrow(z), function(i) {
    tt <- sapply(1:ncol(z), function(j) {
      compute_log_cond_prob(z, i, j, alpha, beta)
    })
    sum(tt)
  })

  return(-sum(result))
}
```


```{r read_results}
grid <- readRDS("grid.rds")
k_result <- readRDS("k_result.rds")
grid.r3 <- readRDS("result_p3.rds")
```

## Problem 1

-   The estimated *h* for the `paulsen` dataset is:

    ```{r, echo=TRUE}
    data("paulsen")
    h.paulsen <- bi_search(paulsen$y, 1, 4, tol = 10^-6)
    h.paulsen
    ```

-   the estimated p-value is:
    
    ```{r, echo=TRUE}
    boot.result <- boot(paulsen$y, boot_paulsen, R = 1000, h = h.paulsen)
    mean(boot.result$t[,1])
    ```

## Problem 2

The probabilities of misclassification are labeled on the plots. According to the plots, we can see that the misclassification probability is large if two or more groups are close to each other. For $K = 3$, large values of $\ddot{\omega}$ make the group more concentrated, while for $K = 4$, $\ddot{\omega} = 0.05$ results in the most spread group distributions.

```{r}
grid$nrstbl_result <- lapply(1:nrow(grid), function(idx) {
  grid$nrstbl_result[[idx]]$k_est <- k_result[[idx]]$Fhat
  grid$nrstbl_result[[idx]]
})

grid.r <- grid %>% mutate(
  p_misclass = purrr::map_dbl(nrstbl_result, function(nn) {
    nn %>%
      nest(k_result = n_rs_cluster:k_est) %>%
      mutate(
        p_est = purrr::map_dbl(k_result, function(kk) {
          mean(kk$k_est)
        }),
        mis_p = 1 - p_est
      ) %>% { sum(.$mis_p) }
  })
)

grid.r$plot_data <- lapply(grid.r$data_result, function(dd) {
  xx <- dd[[1]]
  xx$x <- xx$data[,1]
  xx$y <- xx$data[,2]
  xx %>% dplyr::select(x, y, cluster)
})

dat_text <- grid.r %>% dplyr::select(omega, K_value, p_misclass)

p <- grid.r %>% dplyr::select(omega, K_value, plot_data, p_misclass) %>%
  unnest(plot_data) %>%
  ggplot() +
  geom_point(aes(x = x, y = y, color = factor(cluster)),
             alpha = 0.5, size = 0.5) +
  facet_grid(factor(omega)~factor(K_value)) +
  theme_bw() +
  labs(color = "Cluster") +
  geom_text(data = dat_text,
            mapping = aes(x = Inf, y = -Inf, label = round(p_misclass, 4)),
            hjust = 1, vjust = -1, size = 3)
p
```

## Problem 3

```{r}
p1 <- grid.r3 %>% ggplot() +
  geom_line(aes(x = n, y = est.consistency,
                color = factor(beta))) +
  labs(color = "beta") +
  scale_x_continuous(breaks = unique(grid.r3$n)) +
  theme_bw()

p2 <- grid.r3 %>% filter(beta < 1.5) %>%
  ggplot() +
  geom_line(aes(x = n, y = est.consistency,
                color = factor(beta))) +
  labs(color = "beta") +
  scale_x_continuous(breaks = unique(grid.r3$n)) +
  theme_bw()
```

The following results show the consistency of $(\alpha, \beta)$ for different values of $n$ and $\beta$. 

```{r}
grid.r3 %>% dplyr::select(n, beta, est.consistency)
```

These results are summarized in the following plots:

-   The first plot shows the estimated consistency for all pairs of $n$ and $\beta$. It shows that when $\beta$ increases and $n$ is small, we tend to have very bad estimation. It makes sense because in this case, most samples have the same value, and we can not obtain a reasonable estimation.

    ```{r}
    p1
    ```

-   Then we removed the $\beta$'s with large values from the plot. Again, we observe that it's hard to make estimation for large $\beta$ values and small number observations. However, increasing the sample size improves the consistency of the estimators.

    ```{r}
    p2
    ```

## Problem 4
# Results (we couldn't get many big pistures, when we run this some of the tests cannot get results)
## big picture:
##### test result  a = 0, b = 0  a = 0     b = 0
##### picture 1     reject       reject   accept
##### picture 2     reject       reject
##### picture 4     accept       accept
##### picture 7     reject       reject     
##### picture 8     reject       reject

## small picture:
##### test result  a = 0, b = 0  a = 0     b = 0
#####1              "reject"    "reject"  "reject"
#####2              "reject"    "reject"  "reject"
#####3              "accept"    "reject"  "accept"
#####4              "reject"    "reject"  "accept"
#####5              "accept"    "accept"  "accept"
#####6              "accept"    "reject"  "reject"
#####7              "reject"    "reject"  "reject"
#####8              "reject"    "reject"  "reject"
#####9              "reject"    "reject"  "reject"
#####10             "reject"    "reject"  "reject"
#####11             "accept"    "reject"  "reject"
#####12             "accept"    "reject"  "reject"
#####13             "reject"    "reject"  "reject"
#####14             "reject"    "reject"  "reject"
#####15             "reject"    "reject"  "accept"

## Appendix

`R` codes used for this homework:

```{r codes, echo=TRUE, eval=FALSE}
library(tidyverse)
library(boot)
library(zoo)

data("paulsen")

# p1
k_dens <- function(x, h, data) {
  n <- length(data)
  sum(dnorm((x - data) / h)) / n
}

n_peak <- function(data, h = 1, x_length.out = 100, xmin = -10, xmax = 30) {
  x <- seq(xmin, xmax, length.out = x_length.out)
  y <- sapply(x, k_dens, h = h, data = data)
  check.uni <- sum(zoo::rollapply(y, 3, function(x) {x[2] == max(x)}))
  return(check.uni)
}

bi_search <- function(data, lower, upper, tol = 10^-3){
  ll <- n_peak(data, lower)
  rr <- n_peak(data, upper)

  if(!(ll > 1 & rr == 1)) {
    stop("choose lower and upper again")
  }

  repeat{
    mid <- mean(c(lower, upper))
    mm <- n_peak(data, mid)

    if(mm > 1) {
      lower <- mid
    } else {
      upper <- mid
    }

    if(abs(lower - upper) < tol) break
  }

  return(upper)
}
data("paulsen")
h.paulsen <- bi_search(paulsen$y, 1, 4, tol = 10^-6)
h.paulsen

boot_paulsen <- function(data, i, h) {
    x_star <- data[i] + h * rnorm(length(data))

    n_peak(x_star, h = h) > 1
}

boot.result <- boot(paulsen$y, boot_paulsen, R = 1000, h = h.paulsen)
mean(boot.result$t[,1])

# p2
# library(MixSim)
library(tidyverse)
get_n_rs_tbl <- function(tt, k, center) {
  cc <- combn(k, 2)
  nrstbl <- tibble(cluster1 = c(cc[1,], cc[2,]),
                   cluster2 = c(cc[2,], cc[1,]))
  nrstbl$l_center <- lapply(nrstbl$cluster2, function(idx) center[idx,])
  nrstbl$Xi <- lapply(nrstbl$cluster1, function(cluster) {
    tt$data[tt$cluster == cluster, ]
  })
  nrstbl <- nrstbl %>% mutate(
    n_rs_cluster = purrr::map2(l_center, Xi, function(cent, dd) {
      sapply(1:nrow(dd), function(idx) {
        sqrt(sum((dd[idx, ] - cent)^2))
      })
    })
  ) %>% unnest(n_rs_cluster)
  return(nrstbl)
}

grid <- expand.grid(omega = c(0.005, 0.01, 0.05), K_value = 3:4)
grid <- as_tibble(grid)

grid <- grid %>% mutate(
  data_result = purrr::map2(omega, K_value, function(oo, kk) {
    Q <- MixSim::MixGOM(goMega=oo, p = 2, hom = T, sph = T, K = 4)
    A <- MixSim::simdataset(n = 500, Pi = Q$Pi, Mu = Q$Mu, S = Q$S, n.out = 0, int = c(0, 1))

    dd <- A$X
    k.rr <- kmeans(dd, centers = kk, iter.max = 20, nstart = 10)

    tt <- tibble(data = dd, cluster = k.rr$cluster)
    tt <- tt %>% left_join(tibble(center = k.rr$centers, cluster = 1:nrow(k.rr$centers)),
                           by = "cluster")
    tt$n_rs <- sapply(1:nrow(tt), function(idx) {
      x <- tt$data[idx, ]
      center <- tt$center[idx, ]

      sqrt(sum((x - center)^2))
    })

    list(tt = tt, k.rr = k.rr)
  }),
  nrstbl_result = purrr::map2(K_value, data_result, function(kk, dd) {
    get_n_rs_tbl(dd$tt, k = kk, dd$k.rr$centers)
  })
)

saveRDS(grid, "homework2/grid.rds")

# grid <- readRDS("grid.rds")
# k_result <- lapply(1:nrow(grid), function(idx) {
#   SynClustR::kcdf(grid$data_result[[idx]]$tt$n_rs, xgrid = grid$nrstbl_result[[idx]]$n_rs_cluster)
# })
# saveRDS(k_result, "k_result.rds")

k_result <- readRDS("~/STAT_680/STAT680/homework2/k_result.rds")

grid$nrstbl_result <- lapply(1:nrow(grid), function(idx) {
  grid$nrstbl_result[[idx]]$k_est <- k_result[[idx]]$Fhat
  grid$nrstbl_result[[idx]]
})

grid.r <- grid %>% mutate(
  p_misclass = purrr::map_dbl(nrstbl_result, function(nn) {
    nn %>%
      nest(k_result = n_rs_cluster:k_est) %>%
      mutate(
        p_est = purrr::map_dbl(k_result, function(kk) {
          mean(kk$k_est)
        }),
        mis_p = 1 - p_est
      ) %>% { sum(.$mis_p) }
  })
)

check.idx <- 3
grid.r
grid.r$data_result[[check.idx]]$tt$data %>% { plot(.[,1], .[,2]) }
grid.r$data_result[[check.idx]]$k.rr$centers %>% { points(.[,1], .[,2], col = "red") }

grid.r$plot_data <- lapply(grid.r$data_result, function(dd) {
  xx <- dd[[1]]
  xx$x <- xx$data[,1]
  xx$y <- xx$data[,2]
  xx %>% select(x, y, cluster)
})

dat_text <- grid.r %>% select(omega, K_value, p_misclass)

p <- grid.r %>% select(omega, K_value, plot_data, p_misclass) %>%
  unnest(plot_data) %>%
  ggplot() +
  geom_point(aes(x = x, y = y, color = factor(cluster)),
             alpha = 0.5, size = 0.5) +
  facet_grid(factor(omega)~factor(K_value)) +
  theme_bw() +
  labs(color = "Cluster") +
  geom_text(data = dat_text,
            mapping = aes(x = Inf, y = -Inf, label = round(p_misclass, 4)),
            hjust = 1, vjust = -1, size = 3)
p

# p3
library(bayesImageS)

n <- 50
beta <- 1
mask <- matrix(1,n,n) # basically the grid
neigh <- getNeighbors(mask, c(2,2,0,0)) # the neighborhood structure
# 1st order neighborhood in 2D
block <- getBlocks(mask, 2)
k <- 2 #(number of classes, k=2 makes Potts’ to be an Ising model)
result <- swNoData(beta = beta,k = k,neigh = neigh, block = block)
z <- matrix(max.col(result$z)[1:nrow(neigh)], nrow=nrow(mask))
z <- z - 1
# the output is the last realization, since we only have two
# classes,  one of the $k=2$ columns is adequate.

find_neighbor <- function(z, i, j) {
  dim.z <- dim(z)

  if(i > dim.z[1] | j > dim.z[2]) return(NULL)

  count <- 0

  neigh.idx <- list()
  if(i - 1 >= 1) {
    count <- count + 1
    neigh.idx[[count]] <- c(i-1, j)
  }

  if(i + 1 <= dim.z[1]) {
    count <- count + 1
    neigh.idx[[count]] <- c(i + 1, j)
  }

  if(j - 1 >= 1) {
    count <- count + 1
    neigh.idx[[count]] <- c(i, j - 1)
  }

  if(j + 1 <= dim.z[2]) {
    count <- count + 1
    neigh.idx[[count]] <- c(i, j + 1)
  }

  return(neigh.idx)

}

find_neighbor(z, 49, 49)

compute_log_cond_prob <- function(z, i, j, alpha = 0, beta = 0) {
  xi <- z[i,j]
  xi_neighbor <- sapply(find_neighbor(z, i, j), function(nn) {
    z[nn[1], nn[2]]
  })

  beta_coef <- sum(xi == xi_neighbor)
  num <- xi * alpha + beta_coef * beta
  dem <- log(exp(num) + exp( (1-xi)*alpha + (length(xi_neighbor) - beta_coef)*beta ))

  return(num - dem)
}

compute_neg_log_prof_llh <- function(par, z) {
  alpha <- par[1]
  beta <- par[2]

  result <- sapply(1:nrow(z), function(i) {
    tt <- sapply(1:ncol(z), function(j) {
      compute_log_cond_prob(z, i, j, alpha, beta)
    })
    sum(tt)
  })

  return(-sum(result))
}

n <- 100
beta <- 1
mask <- matrix(1,n,n) # basically the grid
neigh <- getNeighbors(mask, c(2,2,0,0)) # the neighborhood structure
# 1st order neighborhood in 2D
block <- getBlocks(mask, 2)
k <- 2 #(number of classes, k=2 makes Potts’ to be an Ising model)
result <- swNoData(beta = beta,k = k,neigh = neigh, block = block)
z <- matrix(max.col(result$z)[1:nrow(neigh)], nrow=nrow(mask))
z <- z - 1

opt.rr <- optim(par = c(0, 0.5), compute_neg_log_prof_llh, z = z)
eval.consistency <- sqrt(sum((opt.rr$par - c(0, beta))^2))

grid <- expand.grid(beta = c(0, .25, .5, .75, 1, 1.25, 1.5, 1.75),
                    n = c(10, 25, 50, 100))
grid <- grid %>% mutate(
  sim.data = purrr::map2(beta,n, function(bb, nn) {
    mask <- matrix(1,nn,nn) # basically the grid
    neigh <- getNeighbors(mask, c(2,2,0,0)) # the neighborhood structure
    # 1st order neighborhood in 2D
    block <- getBlocks(mask, 2)
    k <- 2 #(number of classes, k=2 makes Potts’ to be an Ising model)
    result <- swNoData(beta = bb,k = k,neigh = neigh, block = block)
    z <- matrix(max.col(result$z)[1:nrow(neigh)], nrow=nrow(mask))
    z <- z - 1
    z
  })
)

grid.r3 <- grid %>% mutate(
  est.par = purrr::map(sim.data, function(z) {
    opt.rr <- optim(par = c(0, 0.75), compute_neg_log_prof_llh, z = z)
    opt.rr$par
  })
)

grid.r3 <- grid.r3 %>% mutate(
  est.consistency = purrr::map2_dbl(beta, est.par, function(bb, pp) {
    sqrt(sum((pp - c(0, bb))^2))
  })
)

opt.rr <- optim(par = c(0, 1.75), compute_neg_log_prof_llh, z = grid.r3$sim.data[[8]])
eval.consistency <- sqrt(sum((opt.rr$par - c(0, 1.75))^2))

saveRDS(grid.r3, "homework2/result_p3.rds")

p1 <- grid.r3 %>% ggplot() +
  geom_line(aes(x = n, y = est.consistency,
                color = factor(beta))) +
  labs(color = "beta") +
  scale_x_continuous(breaks = unique(grid.r3$n)) +
  theme_bw()

p2 <- grid.r3 %>% filter(beta < 1.5) %>%
  ggplot() +
  geom_line(aes(x = n, y = est.consistency,
                color = factor(beta))) +
  labs(color = "beta") +
  scale_x_continuous(breaks = unique(grid.r3$n)) +
  theme_bw()


#p4

count_neighbor_mc <- function(z, cores = 6) {
  grid <- expand.grid(i = 1:dim(z)[1], j = 1:dim(z)[2])
  # grid$xi <- NA
  # grid$beta_coef <- NA
  # grid$type <- NA
  grid.list <- mclapply(1:nrow(grid), function(k) {
    i <- grid$i[k]
    j <- grid$j[k]

    xi <- z[i,j]
    xi_neighbor <- sapply(find_neighbor(z, i, j), function(nn) { z[nn[1], nn[2]] })

    beta_coef <- sum(xi == xi_neighbor)

    type <- length(xi_neighbor)

    c(xi, beta_coef, type)

  }, mc.cores = cores)

  grid <- do.call(rbind, grid.list)
  colnames(grid) <- c("xi", "beta_coef", "type")
  grid <- as_tibble(grid)

  result <- grid %>% group_by(xi, beta_coef, type) %>%
    summarise(
      count = n(),
      .groups = "drop"
    )

  return(result)
}

compute_neg_log_prof_llh.neighbor <- function(par, z.neighbor, alpha_zero = FALSE, beta_zero = FALSE) {
  if(length(par) == 2){
    alpha <- par[1]
    beta <- par[2]
  } else {
    if(alpha_zero & !beta_zero) {
      alpha <- 0
      beta <- par
    } else if (beta_zero & !alpha_zero) {
      alpha <- par
      beta <- 0
    } else{
      alpha <- 0
      beta <- 0
    }
  }

  z.neighbor <- z.neighbor %>% mutate(
    log_cond_prob = purrr::pmap_dbl(list(xi, beta_coef, type), function(xi, beta_coef, type) {
      num <- xi * alpha + beta_coef * beta
      denom <- log(exp(num) + exp( (1-xi) * alpha + (type - beta_coef) * beta ))
      num - denom
    })
  )

  result <- with(z.neighbor, {
    -sum(count * log_cond_prob)
  })

  return(result)
}

compute_MPLE_ratio <- function(dat.neighbor, q) {

  opt.full <- optim(par = c(0.5, 0.5), compute_neg_log_prof_llh.neighbor,
                    z.neighbor = dat.neighbor,
                    method = "L-BFGS-B", lower = c(0.0001, 0.0001))

  if(q == 0){
    null.par <- 0
    null.val <- -sum(dat.neighbor$count) * log(0.5)
  } else if (q == 1) {
    opt.null <- optim(par = c(0.5), compute_neg_log_prof_llh.neighbor,
                 z.neighbor = dat.neighbor, beta_zero = TRUE,
                 method = "L-BFGS-B", lower = 0.0001)
    null.par <- opt.null$par
    null.val <- opt.null$value
  } else {
    opt.null <- optim(par = c(0.5), compute_neg_log_prof_llh.neighbor,
                 z.neighbor = dat.neighbor, alpha_zero = TRUE,
                 method = "L-BFGS-B", lower = 0.0001)
    null.par <- opt.null$par
    null.val <- opt.null$value
  }

  return(list(full.par = opt.full$par, full.val = opt.full$value,
              null.par = null.par, null.val = null.val))
}

fun2 <- function(LSBs, id, layer, B = 50, q = 0, FUN = count_neighbor) {

  dat.original <- LSBs[[id]]$lsb[, , layer]
  dat.original.neighbor <- FUN(dat.original)

  result <- list()
  result[[1]] <- compute_MPLE_ratio(dat.original.neighbor, q)


  if(q == 0) {
    for(b in 1:B) {
      smp = matrix(rbinom(nrow(dat.original)*ncol(dat.original), 1, 0.5), nrow = nrow(dat.original))
      dat.neighbor <- FUN(smp)

      result[[b+1]] <- compute_MPLE_ratio(dat.neighbor, q)
    }

  } else if (q == 1) {
    opt.r <- optim(par = c(0.5), compute_neg_log_prof_llh.neighbor,
                   z.neighbor = dat.original.neighbor, beta_zero = TRUE,
                   method = "L-BFGS-B", lower = 0.0001)
    p = exp(opt.r$par)/(1 + exp(opt.r$par))
    for(b in 1:B) {
      smp = matrix(rbinom(nrow(dat.original)*ncol(dat.original), 1, p), nrow = nrow(dat.original))
      dat.neighbor <- FUN(smp)

      result[[b+1]] <- compute_MPLE_ratio(dat.neighbor, q)
    }
  } else {
    n <- nrow(dat.original)
    m <- ncol(dat.original)
    opt.r <- optim(par = c(0.5), compute_neg_log_prof_llh.neighbor,
                   z.neighbor = dat.original.neighbor, alpha_zero = TRUE,
                   method = "L-BFGS-B", lower = 0.0001)
    beta <- opt.r$par
    mask <- matrix(1,n,m)
    neigh <- getNeighbors(mask, c(2,2,0,0))
    block <- getBlocks(mask, 2)
    k <- 2
    for(b in 1:B){
      result <- swNoData(beta = beta,k = k,neigh = neigh, block = block)
      z <- matrix(max.col(result$z)[1:nrow(neigh)], nrow=nrow(mask))
      smp = z - 1
      dat.neighbor <- FUN(smp)

      result[[b+1]] <- compute_MPLE_ratio(dat.neighbor, q)
    }
  }

  return(result)
}

```



